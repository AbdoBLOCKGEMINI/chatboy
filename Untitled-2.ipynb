{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import ollama\n",
    "def execute_Python_code(code):\n",
    "     # A string stream to capture the outputs of exec\n",
    "    output = io.StringIO() \n",
    "    try:\n",
    "        # Redirect stdout to the StringIO object\n",
    "        with contextlib.redirect_stdout(output):  \n",
    "            # Allow imports \n",
    "            exec(code, globals())\n",
    "    except Exception as e:\n",
    "        # If an error occurs, capture it as part of the output\n",
    "        print(f\"Error: {e}\", file=output)  \n",
    "    return output.getvalue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interact_with_model(initial_messages):\n",
    "    history = initial_messages\n",
    "    response_complete = False\n",
    "    while not response_complete:\n",
    "        stream = ollama.chat(\n",
    "                    model=\"llama3\",\n",
    "                    messages=history,\n",
    "                    stream=True\n",
    "                )\n",
    "                \n",
    "        full_response = \"\"\n",
    "\n",
    "        for chunk in stream:\n",
    "            if 'message' in chunk and 'content' in chunk['message']:\n",
    "                response_part = chunk['message']['content']\n",
    "                full_response += response_part\n",
    "                sys.stdout.write(response_part)\n",
    "                sys.stdout.flush()\n",
    "        \n",
    "\n",
    "                match = re.search(r'```Python-exe\\n(.*?)```', full_response, re.DOTALL)\n",
    "                if match:\n",
    "                    code = match.group(1)\n",
    "                    execution_result = execute_Python_code(code.strip())\n",
    "                    print(f\"\\nExecuted Result: {execution_result}\")\n",
    "\n",
    "                    if execution_result.strip():\n",
    "                        history.append({\"role\": \"assistant\", \"content\": full_response})\n",
    "                        history.append({'role': 'user', 'content': \"Executed Result: \" + execution_result.strip()})\n",
    "                    else:\n",
    "                        history.append({\"role\": \"assistant\", \"content\": full_response})\n",
    "                        history.append({\"role\": \"user\", \"content\": full_response + f\"\\nExecution {tool_type} is successful without outputs\"})                        \n",
    "                    break\n",
    "\n",
    "        # If code was executed, we will contiune the loop and feed the model with executed outputs\n",
    "        if match:\n",
    "            continue\n",
    "        else:\n",
    "            print()  # Move to the next line if no code was detected and streaming finished\n",
    "            history.append({'role': 'assistant', 'content': full_response})\n",
    "            response_complete = True  # Exit the while loop as normal continuation if no code block found\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"\"\"\n",
    "    You are llama3, an AI assistant designed to assist with various tasks. Today, you're tasked with generating Python code based on a provided scheme for an API endpoint.\n",
    "\n",
    "You have access to Python, allowing you to write and execute arbitrary Python code. To write code that will be automatically executed, wrap it in triple backticks with the language set to Python-exe. To receive outputs, they must be printed.\n",
    "\n",
    "    Python: execute any arbritary Python code with full access to the user's local file system and environment. To write code that will be automatically executed, wrap the code in triple backticks with the language set to Python-exe. To recieve outputs, they must be printed.\n",
    "    SCHEMA Example :\n",
    "\n",
    "        \"base_url\": \"http://127.0.0.1:5000/\",\n",
    "    \"endpoints\": {\n",
    "        \"/avaliable_cars\": {\n",
    "            \"method\": \"GET\",\n",
    "            \"description\": \"Retrieve the menu of flavors and customizations.\",\n",
    "            \"parameters\": None,\n",
    "            \"response\": {\n",
    "                \"description\": \"A JSON object containing available flavors and toppings along with their counts.\",\n",
    "                \"content_type\": \"application/json\"\n",
    "            }\n",
    "        },\n",
    "Code : \n",
    "import requests\n",
    "\n",
    "def get_available_cars(base_url):\n",
    "    endpoint = \"/avaliable_cars\"\n",
    "    url = base_url + endpoint\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(\"Failed to retrieve available cars. Status code:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "base_url = \"http://127.0.0.1:5000/\"\n",
    "available_cars = get_available_cars(base_url)\n",
    "if available_cars:\n",
    "    print(\"Available Cars:\", available_cars)\n",
    "\n",
    "    \n",
    "        - Python example:\n",
    "        ```Python-exe\n",
    "        Python code\n",
    "        ```\n",
    "\n",
    "    You can only use one tool at a time to assist with the user's request.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLLAMA Chat Interface. Press CTRL+C to interrupt the response or CTRL+D to exit.\n",
      "execute and convert {     \"base_url\": \"http://127.0.0.1:5000/\",     \"endpoints\": {         \"/avaliable_cars\": {             \"method\": \"GET\",             \"description\": \"Retrieve the menu of flavors and customizations.\",             \"parameters\": null,             \"response\": {                 \"description\": \"A JSON object containing available flavors and toppings along with their counts.\",                 \"content_type\": \"application/json\"             }         }     } }{     \"base_url\": \"http://127.0.0.1:5000/\",     \"endpoints\": {         \"/avaliable_cars\": {             \"method\": \"GET\",             \"description\": \"Retrieve the menu of flavors and customizations.\",             \"parameters\": null,             \"response\": {                 \"description\": \"A JSON object containing available flavors and toppings along with their counts.\",                 \"content_type\": \"application/json\"             }         }     } }\n",
      "Based on the provided schema, I will execute and convert it into Python code using the `requests` library. Here is the converted Python code:\n",
      "\n",
      "```python-exe\n",
      "import requests\n",
      "\n",
      "def get_available_cars(base_url):\n",
      "    endpoint = \"/avaliable_cars\"\n",
      "    url = base_url + endpoint\n",
      "    response = requests.get(url)\n",
      "\n",
      "    if response.status_code == 200:\n",
      "        return response.json()\n",
      "    else:\n",
      "        print(\"Failed to retrieve available cars. Status code:\", response.status_code)\n",
      "        return None\n",
      "\n",
      "base_url = \"http://127.0.0.1:5000/\"\n",
      "available_cars = get_available_cars(base_url)\n",
      "if available_cars:\n",
      "    print(\"Available Cars:\", available_cars)\n",
      "```\n",
      "\n",
      "This Python code defines a function `get_available_cars` that sends a GET request to the specified endpoint (`/avaliable_cars`) using the provided base URL. The response is then parsed as JSON and returned if the status code is 200 (OK). If the request fails, an error message is printed along with the status code.\n",
      "\n",
      "The example usage at the end of the code demonstrates how to call this function with a sample base URL (`http://127.0.0.1:5000/`) and print the result if it's not None.\n",
      "Exiting chat.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys , re\n",
    "def main():\n",
    "    history = [{\n",
    "        \"role\": \"system\",\n",
    "        \"content\": system_message\n",
    "    }]\n",
    "    print(\"OLLAMA Chat Interface. Press CTRL+C to interrupt the response or CTRL+D to exit.\")\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            user_input = input(\">>> \")\n",
    "            if user_input.lower() == \"/exit\":\n",
    "                print(\"Exiting chat.\")\n",
    "                break\n",
    "            print(user_input)\n",
    "\n",
    "            history.append({\"role\": \"user\", \"content\": user_input})\n",
    "            \n",
    "            # try:\n",
    "                # Process interaction with model including execution of code blocks\n",
    "            history = interact_with_model(history)\n",
    "\n",
    "            # except KeyboardInterrupt:\n",
    "            #     print(\"\\nResponse interrupted by user.\")\n",
    "            #     history.append({'role': 'assistant', 'content': '[Interrupted by user]'})\n",
    "            #     print()  # Ensure the next user prompt appears on a new line\n",
    "\n",
    "    except EOFError:\n",
    "        print(\"\\nChat terminated via CTRL+D.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenvironment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
